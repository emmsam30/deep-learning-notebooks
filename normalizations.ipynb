{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6c692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "torch.manual_seed(42)\n",
    "X = torch.rand(1000, 20)\n",
    "y = (X.sum(dim=1) > 10).float()\n",
    "\n",
    "# Split into train and test sets\n",
    "train_X, test_X = X[:800], X[800:]\n",
    "train_y, test_y = y[:800], y[800:]\n",
    "\n",
    "# Define a simple feedforward network with normalization\n",
    "class NormalizedNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, use_batch_norm=True):\n",
    "        super(NormalizedNetwork, self).__init__()\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size) if use_batch_norm else nn.LayerNorm(hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size) if use_batch_norm else nn.LayerNorm(hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, train_X, train_y, test_X, test_y, epochs=20):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    train_losses, test_losses = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(train_X)\n",
    "        loss = criterion(outputs.squeeze(), train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(test_X)\n",
    "            test_loss = criterion(test_outputs.squeeze(), test_y)\n",
    "            test_losses.append(test_loss.item())\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "# Initialize models with BatchNorm and LayerNorm\n",
    "batch_norm_model = NormalizedNetwork(input_size=20, hidden_size=64, use_batch_norm=True)\n",
    "layer_norm_model = NormalizedNetwork(input_size=20, hidden_size=64, use_batch_norm=False)\n",
    "\n",
    "# Train both models\n",
    "batch_norm_train_losses, batch_norm_test_losses = train_model(batch_norm_model, train_X, train_y, test_X, test_y)\n",
    "layer_norm_train_losses, layer_norm_test_losses = train_model(layer_norm_model, train_X, train_y, test_X, test_y)\n",
    "\n",
    "# Plot the training and test losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(batch_norm_train_losses, label='BatchNorm Train Loss')\n",
    "plt.plot(batch_norm_test_losses, label='BatchNorm Test Loss')\n",
    "plt.plot(layer_norm_train_losses, label='LayerNorm Train Loss')\n",
    "plt.plot(layer_norm_test_losses, label='LayerNorm Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('BatchNorm vs LayerNorm Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
